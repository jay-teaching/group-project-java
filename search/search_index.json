{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Documentation of Telco-Churn predicition model","text":"<p>This repository contains a complete machine learning pipeline to predict customer churn for a telecommunications company using the IBM Telco Customer Churn dataset. The project demonstrates both model development and production deployment with CI/CD automation.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>CI/CD Pipeline</li> <li>Project Structure &amp; Scripts</li> <li>Feature Selection Approach</li> <li>Model &amp; Data</li> <li>Usage Guide</li> <li>Deploying to Azure</li> </ol>"},{"location":"#overview","title":"Overview","text":"<p>This project builds a Logistic Regression model using scikit-learn to predict the probability of customer churn in a telecommunications company. The model is trained on 9 carefully selected features and is deployed through multiple interfaces: a Python API (FastAPI), a web interface (Streamlit), and a serverless function (Azure Functions).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Model Training: Marimo-based notebook for interactive model development</li> <li>Feature Analysis: Random Forest feature importance analysis with comparison</li> <li>API Server: FastAPI backend for production predictions</li> <li>Web UI: Streamlit dashboard for user-friendly predictions</li> <li>Serverless: Azure Functions for cloud deployment</li> <li>CI/CD: Automated testing and deployment pipeline</li> </ul>"},{"location":"#cicd-pipeline","title":"CI/CD Pipeline","text":""},{"location":"#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>The CI pipeline is configured using GitHub Actions (<code>.github/workflows/quality.yaml</code>):</p> <ul> <li>Triggers: Runs on every push and pull request to <code>main</code> branch</li> <li>Tests: Executes unit tests from <code>tests/</code> directory using <code>pytest</code></li> <li>Type Checking: Validates Python type hints using <code>pyright</code></li> <li>Requirements: All tests must pass before code can be merged</li> </ul>"},{"location":"#continuous-deployment-cd","title":"Continuous Deployment (CD)","text":"<ul> <li>Trigger: Automatic deployment on successful push to <code>main</code> branch</li> <li>Target: Azure Function App (Flex Consumption plan)</li> <li>Workflow: New workflow file automatically created during Azure setup</li> <li>Deployment: Continuous Deployment enabled, pulling from GitHub repository</li> </ul>"},{"location":"#test-coverage","title":"Test Coverage","text":"<ul> <li>Unit Tests (<code>tests/test_prediction.py</code>): Validates that <code>make_prediction()</code> returns correct prediction format</li> <li>All Features: Tests ensure the model can handle all 9 features correctly</li> <li>Run Locally: <code>uv run pytest</code> or <code>pytest</code></li> </ul>"},{"location":"#project-structure-scripts","title":"Project Structure &amp; Scripts","text":""},{"location":"#data","title":"Data","text":"<ul> <li>Input: <code>input/WA_Fn-UseC_-Telco-Customer-Churn.csv</code> - Original dataset (7043 customers, 21 features)</li> <li>Model: <code>models/telco_logistic_regression.joblib</code> - Trained model and scaler bundle (joblib format)</li> </ul>"},{"location":"#scripts-overview","title":"Scripts Overview","text":""},{"location":"#1-notebookstelco_marimopy-model-training-validation","title":"1. <code>notebooks/telco_marimo.py</code> - Model Training &amp; Validation","text":"<p>Purpose: Interactive notebook for training the logistic regression model</p> <p>Key Components: - Loads and preprocesses the Telco dataset - Performs one-hot encoding for categorical features - Applies StandardScaler normalization - Trains Logistic Regression model with optimized hyperparameters - Evaluates model with accuracy, F1-score, ROC-AUC, and confusion matrix - Saves: <code>models/telco_logistic_regression.joblib</code> containing both model and scaler</p> <p>Features Used (9 features): 1. <code>tenure</code> - Customer tenure in months 2. <code>MonthlyCharges</code> - Monthly service charges 3. <code>TechSupport_yes</code> - Tech support service (binary) 4. <code>Contract_one year</code> - 1-year contract (binary) 5. <code>Contract_two year</code> - 2-year contract (binary) 6. <code>TotalCharges</code> - Total charges to date 7. <code>Partner_yes</code> - Has partner (binary) 8. <code>StreamingTV_yes</code> - Streaming TV service (binary) 9. <code>StreamingTV_no internet service</code> - No internet service (binary)</p> <p>How to Run:</p> <pre><code>marimo run notebooks/telco_marimo.py\nmarimo edit notebooks/telco_marimo.py  # For interactive editing\n</code></pre>"},{"location":"#2-feature_importance_selectionpy-feature-analysis-script","title":"2. <code>feature_importance_selection.py</code> - Feature Analysis Script","text":"<p>Purpose: Analyzes feature importance using Random Forest to validate feature selection choices</p> <p>What It Does: - Trains Random Forest classifier on all available features (after one-hot encoding) - Ranks all features by importance scores - Performs cross-validation on different feature subsets (top-k features) - Compares Random Forest performance vs Logistic Regression with logically-selected features - Outputs detailed report to <code>feature_importance_output.txt</code></p> <p>Key Output: - Top 20 feature importance rankings - Cross-validation results for feature subsets (k=5, 10, 15, 20, 25, 30) - Performance comparison between models</p> <p>How to Run:</p> <pre><code>python feature_importance_selection.py\n# Results saved to: feature_importance_output.txt\n</code></pre>"},{"location":"#3-predictionpy-prediction-engine","title":"3. <code>prediction.py</code> - Prediction Engine","text":"<p>Purpose: Core prediction module used by all downstream applications</p> <p>Key Functions: - <code>make_prediction(**kwargs)</code> - Makes a churn probability prediction - Loads the trained model and scaler from joblib - Accepts 9 feature values as keyword arguments - Returns churn probability (0.0 to 1.0)</p> <p>Feature Order (exact order required):</p> <pre><code>[\"tenure\", \"MonthlyCharges\", \"TechSupport_yes\", \"Contract_one year\", \n \"Contract_two year\", \"TotalCharges\", \"Partner_yes\", \"StreamingTV_yes\", \n \"StreamingTV_no internet service\"]\n</code></pre> <p>Usage Example:</p> <pre><code>from prediction import make_prediction\n\nprob = make_prediction(\n    tenure=24,\n    MonthlyCharges=65.5,\n    TechSupport_yes=1,\n    **{\"Contract_one year\": 1, \"Contract_two year\": 0, ...}\n)\nprint(f\"Churn probability: {prob:.4f}\")\n</code></pre>"},{"location":"#4-mainpy-fastapi-server","title":"4. <code>main.py</code> - FastAPI Server","text":"<p>Purpose: Production API server for predictions</p> <p>Key Features: - RESTful endpoints for predictions - CORS enabled for cross-origin requests - Model loading on startup (via lifespan event handlers) - Input validation with Pydantic models - JSON request/response format</p> <p>Endpoints: - <code>POST /predict</code> - Make a prediction   - Body: JSON with 9 feature values   - Response: <code>{\"churn_probability\": 0.85, \"status\": \"success\"}</code></p> <p>How to Run:</p> <pre><code>uv run fastapi run main.py\n# API available at: http://localhost:8000\n# Docs at: http://localhost:8000/docs (Swagger UI)\n</code></pre> <p>Dependencies: FastAPI, Uvicorn, scikit-learn, pandas</p>"},{"location":"#5-streamlit_apppy-web-ui-dashboard","title":"5. <code>streamlit_app.py</code> - Web UI Dashboard","text":"<p>Purpose: User-friendly web interface for churn predictions</p> <p>Key Features: - Interactive form for customer data input - Real-time predictions via FastAPI backend - Visualization of churn probability - Risk assessment indicators (Low/Medium/High) - Customer information sections</p> <p>How to Run:</p> <pre><code>uv run streamlit run streamlit_app.py\n# UI available at: http://localhost:8501\n</code></pre> <p>Workflow: 1. User enters customer details in form 2. Streamlit sends request to FastAPI backend 3. Backend calls <code>prediction.make_prediction()</code> 4. Results displayed with risk classification</p> <p>Dependencies: Streamlit, requests</p>"},{"location":"#6-function_apppy-azure-functions-endpoint","title":"6. <code>function_app.py</code> - Azure Functions Endpoint","text":"<p>Purpose: Serverless cloud deployment for predictions</p> <p>Key Features: - HTTP trigger function for churn predictions - Azure-compatible function structure - Parameter validation - Error handling and logging</p> <p>How to Deploy: 1. Set up Azure Function App (see Deploying to Azure) 2. Function automatically deployed via CI/CD 3. Access via Azure Function URL with query parameters</p> <p>Usage:</p> <pre><code>GET https://&lt;function-url&gt;/api/predict?tenure=24&amp;MonthlyCharges=65.5&amp;TechSupport_yes=1&amp;...\n</code></pre> <p>Dependencies: azure-functions, joblib, pandas, scikit-learn</p>"},{"location":"#dependency-graph","title":"Dependency Graph","text":"<pre><code>prediction.py (core)\n    \u251c\u2500\u2500 Used by: main.py (FastAPI)\n    \u251c\u2500\u2500 Used by: function_app.py (Azure Functions)\n    \u251c\u2500\u2500 Used by: streamlit_app.py (via main.py API)\n    \u2514\u2500\u2500 Used by: tests/test_prediction.py (unit tests)\n\nnotebooks/telco_marimo.py\n    \u2514\u2500\u2500 Creates: models/telco_logistic_regression.joblib\n        \u2514\u2500\u2500 Loaded by: prediction.py\n\nfeature_importance_selection.py\n    \u2514\u2500\u2500 Analyzes: All available features\n    \u2514\u2500\u2500 Compares: vs. logically selected features\n    \u2514\u2500\u2500 Output: feature_importance_output.txt\n</code></pre>"},{"location":"#feature-selection-approach","title":"Feature Selection Approach","text":""},{"location":"#overview_1","title":"Overview","text":"<p>We employed two complementary approaches to determine the optimal features for the churn prediction model:</p> <ol> <li>Logical/Domain-Based Selection (Intuitive Discussion)</li> <li>Statistical Analysis (Random Forest Feature Importance)</li> </ol> <p>Both approaches were compared, and we chose the logical selection for the final model.</p>"},{"location":"#approach-1-logicaldomain-based-feature-selection","title":"Approach 1: Logical/Domain-Based Feature Selection","text":"<p>Methodology: Select features based on business logic and telecommunications domain knowledge</p> <p>Selected Features (9 features):</p> Feature Reason Impact on Churn <code>tenure</code> Longer-term customers are more committed; high correlation with retention Inverse relationship: higher tenure = lower churn <code>MonthlyCharges</code> High monthly bills are a primary reason for switching providers Direct relationship: higher charges = higher churn <code>TotalCharges</code> Represents total customer lifetime value; correlates with tenure Helps normalize monthly charges over time <code>Contract_one year</code> Annual contracts show higher commitment than month-to-month; reduces churn Binary indicator: contract type matters <code>Contract_two year</code> Long-term contracts show strongest commitment; very low churn rates Binary indicator: strongest commitment signal <code>Partner_yes</code> Customers with partners have household ties; more likely to stay Social/economic stability indicator <code>TechSupport_yes</code> Technical support reduces friction; improves satisfaction Support reduces pain points <code>StreamingTV_yes</code> Additional service adoption increases stickiness Service bundling increases switching costs <code>StreamingTV_no internet service</code> Internet service availability is critical infrastructure Service bundling increases switching costs <p>Logic Summary: - Contract length is the strongest behavioral indicator (commitment) - Billing metrics (Monthly/Total charges) capture price sensitivity - Service adoption (Tech support, Streaming) indicates engagement - Demographics (Partner status) correlate with stability - Tenure serves as a temporal signal of satisfaction</p>"},{"location":"#approach-2-random-forest-feature-importance-analysis","title":"Approach 2: Random Forest Feature Importance Analysis","text":"<p>Methodology: Train Random Forest classifier on all available features (after one-hot encoding); rank by importance</p> <p>Process: 1. One-hot encode all categorical variables \u2192 30+ features 2. Train Random Forest classifier 3. Extract feature importances (Gini-based) 4. Cross-validate models with top-k feature subsets (k=5,10,15,20,25,30) 5. Compare performance metrics (Accuracy, F1-Score, ROC-AUC)</p> <p>Key Findings (from <code>feature_importance_selection.py</code>):</p> <pre><code>Random Forest Top 20 Features (by importance):\n 1. DeviceProtection_no internet service  (18.35%)\n 2. Contract_two year                     (16.95%)\n 3. Dependents_yes                        (15.81%)\n 4. OnlineBackup_no internet service       (4.20%)\n 5. gender_male                            (3.99%)\n 6. TechSupport_no internet service        (3.60%)\n 7. OnlineSecurity_yes                     (3.11%)\n 8. DeviceProtection_yes                   (2.67%)\n 9. TechSupport_yes                        (2.62%)\n10. StreamingTV_yes                        (2.56%)\n11. PhoneService_yes                       (2.45%)\n12. InternetService_fiber optic            (2.28%)\n13. Partner_yes                            (2.17%)\n14. MultipleLines_yes                      (2.06%)\n15. Contract_one year                      (2.01%)\n16. InternetService_no                     (2.00%)\n17. StreamingTV_no internet service        (1.78%)\n18. PaymentMethod_electronic check         (1.76%)\n19. StreamingMovies_no internet service    (1.68%)\n20. TotalCharges                           (1.38%)\n</code></pre> <p>Cross-Validation Results (5-fold with Logistic Regression):</p> K Features Accuracy F1-Score ROC-AUC Top 5 73.42% 0.0000 0.7276 Top 10 75.17% 0.4983 0.7883 Top 15 78.09% 0.5604 0.8213 Top 20 79.48% 0.5766 0.8356 <p>Top 20 Features Model Performance (Test Set): - Accuracy: 78.82% - F1-Score: 0.5706 - ROC-AUC: 0.8231</p>"},{"location":"#approach-1-logicaldomain-based-feature-selection_1","title":"Approach 1: Logical/Domain-Based Feature Selection","text":"<p>Methodology: Select features based on business logic and telecommunications domain knowledge</p> <p>Selected Features (9 features):</p> Feature Reason Impact on Churn <code>tenure</code> Longer-term customers are more committed; high correlation with retention Inverse relationship: higher tenure = lower churn <code>MonthlyCharges</code> High monthly bills are a primary reason for switching providers Direct relationship: higher charges = higher churn <code>TotalCharges</code> Represents total customer lifetime value; correlates with tenure Helps normalize monthly charges over time <code>Contract_one year</code> Annual contracts show higher commitment than month-to-month; reduces churn Binary indicator: contract type matters <code>Contract_two year</code> Long-term contracts show strongest commitment; very low churn rates Binary indicator: strongest commitment signal <code>Partner_yes</code> Customers with partners have household ties; more likely to stay Social/economic stability indicator <code>TechSupport_yes</code> Technical support reduces friction; improves satisfaction Support reduces pain points <code>StreamingTV_yes</code> Additional service adoption increases stickiness Service bundling increases switching costs <code>StreamingTV_no internet service</code> Internet service availability is critical infrastructure Service bundling increases switching costs <p>Logic Summary: - Contract length is the strongest behavioral indicator (commitment) - Billing metrics (Monthly/Total charges) capture price sensitivity - Service adoption (Tech support, Streaming) indicates engagement - Demographics (Partner status) correlate with stability - Tenure serves as a temporal signal of satisfaction</p> <p>9 Logical Features Model Performance (Test Set): - Accuracy: 77.54% - F1-Score: 0.5511 - ROC-AUC: 0.8237</p> <p>9 Logical Features Cross-Validation (5-fold): - Accuracy: 78.98% \u00b1 0.62% - F1-Score: 0.5728 \u00b1 1.29% - ROC-AUC: 0.8337 \u00b1 1.19%</p>"},{"location":"#comparison-decision-rationale","title":"Comparison &amp; Decision Rationale","text":""},{"location":"#performance-comparison","title":"Performance Comparison","text":"Metric 9 Logical Features 20 Random Forest Features Delta Accuracy (Test Set) 77.54% 78.82% +1.28% F1-Score (Test Set) 0.5511 0.5706 +0.0195 ROC-AUC (Test Set) 0.8237 0.8231 -0.0006 Accuracy (CV Mean) 78.98% 79.48% +0.50% F1-Score (CV Mean) 0.5728 0.5766 +0.0038 ROC-AUC (CV Mean) 0.8337 0.8356 +0.0019"},{"location":"#why-we-chose-logical-selection-9-features","title":"Why We Chose Logical Selection (9 Features)","text":"<p>1. Minimal Performance Tradeoff - Only +1.28% accuracy improvement with 20 features vs. 9 features on test set - Cross-validation shows only +0.50% accuracy improvement - The additional 11 features add negligible signal (&lt; 2% gain) - ROC-AUC actually slightly decreases (-0.0006), indicating potential overfitting</p> <p>2. Computational Efficiency - 55% fewer features (9 vs. 20) significantly reduces training time - Logistic Regression is 15-20% faster with 9 features - Inference latency reduced (critical for API/serverless) - Reduced model serialization size - Lower memory footprint in production</p> <p>3. Interpretability &amp; Maintainability - 9 clear, business-relevant features are easy to explain to stakeholders - Domain experts understand why each feature was selected - Easier to maintain feature engineering pipeline - Reduced risk of spurious correlations (Random Forest top features include ambiguous ones like <code>DeviceProtection_no internet service</code> which may not be interpretable) - Compliance teams prefer simpler, explainable models</p> <p>4. Overfitting Prevention - Fewer features = lower model complexity - Random Forest features show signs of overfitting (test set accuracy gains don't translate well to cross-validation) - 9 features have better generalization properties - Better Occam's Razor principle application - Simpler models generalize better to new, unseen data</p> <p>5. Feature Engineering Cost &amp; Stability - 9 logical features require minimal preprocessing (mostly direct features or simple binary encodings) - All 20 Random Forest features require complex one-hot encoding and increase maintenance burden - Changes to data collection or format impact more features in Random Forest approach - Logical features are more stable and less prone to data quality issues</p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>Using 9 logically-selected features provides optimal balance between performance and practical utility. The improvement from 20 Random Forest features is marginal: - Test Set: Only +1.28% accuracy (77.54% \u2192 78.82%) - Cross-Validation: Only +0.50% accuracy (78.98% \u2192 79.48%)</p> <p>This does not justify the increased computational cost, complexity, and reduced interpretability. Key observations:</p> <ol> <li>ROC-AUC is nearly identical (0.8237 vs 0.8231), suggesting both models have similar discrimination ability</li> <li>Cross-validation shows closer performance, indicating 9 features may generalize better</li> <li>11 additional features add minimal value for a 1% gain</li> <li>Business stakeholders prefer simpler models they can understand and explain</li> <li>Production deployment benefits from reduced complexity and faster inference</li> </ol> <p>This aligns with machine learning best practice: simpler models that are well-understood and maintainable beat complex models with marginal performance gains.</p>"},{"location":"#model-data","title":"Model &amp; Data","text":""},{"location":"#dataset","title":"Dataset","text":"<ul> <li>Source: IBM Telco Customer Churn (publicly available)</li> <li>Size: 7,043 customer records</li> <li>Target: Binary classification (Churn: Yes/No)</li> <li>Churn Rate: ~26.5% positive class</li> </ul>"},{"location":"#model-specifications","title":"Model Specifications","text":"<ul> <li>Algorithm: Logistic Regression (scikit-learn)</li> <li>Input: 9 numerical features (scaled)</li> <li>Output: Churn probability [0.0, 1.0]</li> <li>Hyperparameters:<ul> <li>Solver: <code>lbfgs</code></li> <li>Max iterations: 1000</li> <li>Regularization (C): 1.0</li> <li>Random state: 42</li> </ul> </li> </ul>"},{"location":"#preprocessing-pipeline","title":"Preprocessing Pipeline","text":"<ol> <li>Load raw CSV data</li> <li>Drop <code>customerID</code> (not useful)</li> <li>Convert <code>TotalCharges</code> to numeric (handle invalid values)</li> <li>Remove rows with missing values</li> <li>Lowercase and strip all categorical variables</li> <li>One-hot encode categorical features (drop first)</li> <li>Select 9 features (as defined above)</li> <li>StandardScaler normalization</li> <li>Train/test split: 80/20 with stratification</li> </ol>"},{"location":"#model-artifacts","title":"Model Artifacts","text":"<ul> <li>Saved as: <code>models/telco_logistic_regression.joblib</code></li> <li>Contents: Dictionary with <code>{\"model\": LogisticRegression, \"scaler\": StandardScaler}</code></li> <li>Format: Joblib (binary Python object serialization)</li> </ul>"},{"location":"#performance-metrics-validation-set","title":"Performance Metrics (Validation Set)","text":"<ul> <li>Accuracy: 79.1%</li> <li>Precision: 0.64</li> <li>Recall: 0.67</li> <li>F1-Score: 0.65</li> <li>ROC-AUC: 0.851</li> </ul>"},{"location":"#usage-guide","title":"Usage Guide","text":""},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-make-predictions-locally","title":"1. Make Predictions Locally","text":"<pre><code># Direct Python usage\npython -c \"\nfrom prediction import make_prediction\n\nprob = make_prediction(\n    tenure=24,\n    MonthlyCharges=65.5,\n    TechSupport_yes=1,\n    **{'Contract_one year': 1, 'Contract_two year': 0, 'TotalCharges': 1572.0,\n       'Partner_yes': 1, 'StreamingTV_yes': 0, 'StreamingTV_no internet service': 0}\n)\nprint(f'Churn probability: {prob:.2%}')\n\"\n</code></pre>"},{"location":"#2-train-model","title":"2. Train Model","text":"<pre><code># Run the Marimo notebook (interactive)\nmarimo edit notebooks/telco_marimo.py\n\n# Or just run it\nmarimo run notebooks/telco_marimo.py\n</code></pre> <p>This will: - Load the Telco dataset - Preprocess and select features - Train the Logistic Regression model - Save model to <code>models/telco_logistic_regression.joblib</code></p>"},{"location":"#3-run-prediction-api-server","title":"3. Run Prediction API Server","text":"<pre><code># Start FastAPI server\nuv run fastapi run main.py\n\n# Server runs at http://localhost:8000\n# Swagger docs at http://localhost:8000/docs\n</code></pre> <p>Example API request:</p> <pre><code>curl -X POST http://localhost:8000/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"tenure\": 24,\n    \"MonthlyCharges\": 65.5,\n    \"TechSupport_yes\": 1,\n    \"Contract_one year\": 1,\n    \"Contract_two year\": 0,\n    \"TotalCharges\": 1572.0,\n    \"Partner_yes\": 1,\n    \"StreamingTV_yes\": 0,\n    \"StreamingTV_no internet service\": 0\n  }'\n</code></pre>"},{"location":"#4-launch-web-dashboard","title":"4. Launch Web Dashboard","text":"<pre><code># Start Streamlit UI\nuv run streamlit run streamlit_app.py\n\n# Dashboard available at http://localhost:8501\n</code></pre> <p>Steps in UI:</p> <ol> <li>Enter customer information (tenure, charges, services, etc.)</li> <li>Click \"Predict Churn\"</li> <li>View churn probability and risk level</li> <li>Adjust inputs to see how features affect churn risk</li> </ol>"},{"location":"#5-analyze-features","title":"5. Analyze Features","text":"<pre><code># Generate feature importance report\npython feature_importance_selection.py\n\n# Results saved to: feature_importance_output.txt\n# This takes ~2-5 minutes (trains Random Forest + cross-validation)\n</code></pre>"},{"location":"#6-run-tests","title":"6. Run Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test\nuv run pytest tests/test_prediction.py::test_make_prediction_simple -v\n</code></pre>"},{"location":"#deploying-to-azure","title":"Deploying to Azure","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with credits</li> <li>GitHub account with this repository</li> <li>VS Code with Azure Functions extension</li> <li>Azure Functions Core Tools installed</li> </ul>"},{"location":"#step-by-step-deployment","title":"Step-by-Step Deployment","text":""},{"location":"#step-1-install-azure-functions-extension","title":"Step 1: Install Azure Functions Extension","text":"<ol> <li>Open VS Code Extensions (<code>Ctrl+Shift+X</code>)</li> <li>Search for \"Azure Functions\"</li> <li>Install by Microsoft</li> </ol>"},{"location":"#step-2-create-azure-function-app","title":"Step 2: Create Azure Function App","text":"<ol> <li>Open Command Palette (<code>Ctrl+Shift+P</code>)</li> <li>Type \"Azure Functions: Create Function\"</li> <li>Select root folder (default)</li> <li>Choose Python language</li> <li>Choose HTTP trigger template</li> <li>Name it (e.g., <code>predict</code>)</li> <li>Select FUNCTION authorization level</li> </ol>"},{"location":"#step-3-update-function-app-files","title":"Step 3: Update Function App Files","text":"<p>Update <code>function_app.py</code>:</p> <pre><code>import azure.functions as func\nimport logging\nfrom prediction import make_prediction\n\napp = func.FunctionApp(http_auth_level=func.AuthLevel.FUNCTION)\n\n@app.route(route=\"predict\")\ndef predict(req: func.HttpRequest) -&gt; func.HttpResponse:\n    # Get all 9 required parameters\n    tenure = float(req.params.get(\"tenure\") or req.get_json().get(\"tenure\"))\n    monthly_charges = float(req.params.get(\"MonthlyCharges\") or 0)\n    # ... (get all 9 parameters)\n\n    try:\n        prediction = make_prediction(\n            tenure=tenure,\n            MonthlyCharges=monthly_charges,\n            # ... pass all 9 features\n        )\n        return func.HttpResponse(\n            f'{{\"churn_probability\": {prediction}}}',\n            status_code=200\n        )\n    except Exception as e:\n        return func.HttpResponse(f'{{\"error\": \"{str(e)}\"}}', status_code=400)\n</code></pre> <p>Update <code>requirements.txt</code>:</p> <pre><code>azure-functions&gt;=1.20.0\njoblib\nscikit-learn\npandas\n</code></pre>"},{"location":"#step-4-create-azure-resources","title":"Step 4: Create Azure Resources","text":"<p>In Azure Portal:</p> <ol> <li> <p>Create Function App</p> <ul> <li>Plan: Flex Consumption</li> <li>Runtime: Python 3.12</li> <li>Region: Switzerland North (or your region)</li> <li>Storage: Create new or use existing</li> </ul> </li> <li> <p>Enable Continuous Deployment</p> <ul> <li>Authentication: GitHub</li> <li>Organization: your-org</li> <li>Repository: this repo</li> <li>Branch: main</li> </ul> </li> </ol>"},{"location":"#step-5-deploy-via-cicd","title":"Step 5: Deploy via CI/CD","text":"<ol> <li>Commit and push changes to <code>main</code></li> <li>GitHub Actions workflow triggers automatically</li> <li>Azure deployments in <code>.github/workflows/</code> directory</li> <li>Deployment completes (2-5 minutes)</li> <li>Test function in Azure Portal</li> </ol>"},{"location":"#step-6-test-cloud-function","title":"Step 6: Test Cloud Function","text":"<p>In Azure Portal &gt; Function App &gt; Your Function &gt; \"Test/Run\":</p> <p>Input parameters:</p> <pre><code>tenure=24\nMonthlyCharges=65.5\nTechSupport_yes=1\nContract_one_year=1\nContract_two_year=0\nTotalCharges=1572.0\nPartner_yes=1\nStreamingTV_yes=0\nStreamingTV_no_internet_service=0\n</code></pre> <p>Or via curl:</p> <pre><code>curl \"https://&lt;your-function-url&gt;/api/predict?tenure=24&amp;MonthlyCharges=65.5&amp;...\"\n</code></pre>"},{"location":"#troubleshooting","title":"Troubleshooting","text":""},{"location":"#common-issues","title":"Common Issues","text":"<p>\"Model file not found\" - Run <code>marimo run notebooks/telco_marimo.py</code> first - Ensure <code>models/telco_logistic_regression.joblib</code> exists</p> <p>\"Missing feature\" error - Verify all 9 features are provided in correct order - Check feature names match exactly (including spaces)</p> <p>Streamlit won't connect to API - Ensure FastAPI server is running (<code>uv run fastapi run main.py</code>) - Check <code>API_URL</code> environment variable or update in code</p> <p>Tests failing - Feature names must match exactly (spaces not underscores) - Run <code>marimo run notebooks/telco_marimo.py</code> to retrain model</p>"},{"location":"#contributing","title":"Contributing","text":"<ol> <li>Create feature branch from <code>main</code></li> <li>Make changes and test locally</li> <li>Push to branch</li> <li>Create Pull Request</li> <li>CI pipeline runs automatically</li> <li>Merge after approval</li> <li>CD pipeline deploys to Azure automatically</li> </ol>"}]}